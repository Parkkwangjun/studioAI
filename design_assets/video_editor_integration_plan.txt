You are an autonomous senior full-stack engineer working inside Google AI Studio Antigravity.

You are helping me integrate the open-source VideoSOS project (Next.js + React + Remotion + FFmpeg.wasm AI video editor) into my existing app, and extend it into a full-featured multi-track video editor.

────────────────────────
ABOUT MY APP (HOST APP)
────────────────────────

My app is an all-in-one content creation studio that already supports:

- Script generation
- Audio/TTS generation (Google TTS, ElevenLabs)
- Image generation
- Video generation
- Extra AI tools:
  - Single image generation
  - Single video generation
  - SFX (sound effects) generation
  - BGM (background music) generation
- Asset management:
  - All generated media assets are stored in a Library
  - Storage & DB are backed by Supabase (or an equivalent backend).

IMPORTANT:
- All heavy AI generation (images, videos, audio) is done via external APIs, NOT on my local machine.
- The timeline editor is responsible ONLY for COMPOSING existing assets:
  - Cutting, reordering, overlaying, adding text & shapes, simple effects, audio mixing, etc.

────────────────────────
GLOBAL PERFORMANCE & RENDERING CONSTRAINTS
────────────────────────

My local PC is low-spec (cheap ~50만원 machine). You MUST design the system so that performance does NOT depend on my local machine.

Therefore:

1. DO NOT use client-side FFmpeg.wasm as the main engine for final video export.
   - Client-side FFmpeg.wasm may be used for tiny utilities or previews at most, but NOT for full-length, full-quality rendering.
   - A situation where a 1-minute video takes many minutes to render in the browser, or stays at 0% for a long time, is NOT acceptable.

2. FINAL RENDERING MUST BE DONE ON A SERVER OR EXTERNAL WORKER USING NATIVE FFmpeg (CPU is fine):
   - Design a backend (or worker service) that:
     - Receives a structured timeline description (JSON) and asset URLs.
     - Uses a native FFmpeg binary (or equivalent) to compose the final video.
   - Optionally, this backend can live on:
     - A CPU-based server
     - A cloud instance
     - Or an external service (e.g., RunPod, GPU, etc.) if we later decide to add heavier AI or GPU features.
   - For now, assume a CPU FFmpeg pipeline is sufficient for our use case:
     - The assets are pre-generated via APIs.
     - The final render is “just” stitching/compositing/cutting/overlaying.

3. TIMELINE EDITOR = PREVIEW & EDITING ONLY:
   - The editor should:
     - Provide responsive, interactive editing UX.
     - Use efficient preview strategies (downscaled resolution, partial segments, etc.).
   - It MUST NOT block the browser for long-running full exports.

4. ASYNCHRONOUS RENDERING PIPELINE:
   - Final exports should be treated as async jobs:
     - Frontend sends a render request → gets a job ID.
     - Frontend polls or subscribes for progress.
     - When done, backend provides a final video URL (e.g., in Supabase Storage).
   - The frontend UI:
     - Shows status (queued, processing, done, error).
     - Never appears stuck at 0% without feedback.

5. DOWNLOADS:
   - The final rendered video will be downloaded to THE USER’S PC as a normal file download.
   - My local machine only:
     - Runs the web UI
     - Sends API requests
     - Downloads the final result.

────────────────────────
VIDEOSOS INTEGRATION CONTEXT
────────────────────────

Project layout (inside my repo):

- `/src/...`         → My existing app (host app)
- `/videosos/src/...` → Original VideoSOS app (cloned as a subfolder, NOT as a separate running app)

You MUST:

- Treat `/videosos` as a **reference and donor** of:
  - Multi-track timeline UI
  - Track/clip models
  - Editor interaction patterns (drag & drop, trim, etc.)
  - Any useful Remotion integration patterns
- You MUST NOT:
  - Attempt to run VideoSOS as a second full app inside mine.
  - Blindly copy its entire rendering pipeline.
  - Depend on its client-side FFmpeg.wasm implementation for final exports.

All new editor-related code must live under **my app’s `/src/...` tree**, follow my naming conventions, and integrate with my asset model and Supabase backend.

────────────────────────
EDITOR GOAL — FEATURES I WANT
────────────────────────

The integrated editor should provide:

1. MULTI-TRACK TIMELINE
   - Separate tracks/rows for:
     - Video clips
     - Image overlays
     - Audio (voice/TTS, BGM, SFX)
     - Text overlays
     - Shape overlays (boxes, highlight rectangles, etc.)

2. BASIC EDITING UX
   - Drag & drop:
     - From my Library panel onto the timeline (video, image, audio assets).
   - Trim:
     - Adjust start/end of a clip via handles.
   - Move:
     - Drag clips along the time axis and between tracks.
   - Split:
     - Split a clip at the playhead.
   - Copy & paste:
     - Duplicate clips.
   - Timeline navigation:
     - Horizontal scroll (drag or scrollbar)
     - Zoom in/out (e.g. mouse wheel).

3. TEXT OVERLAYS
   - Add “Text” clips on a dedicated track.
   - Properties:
     - content (string)
     - font family
     - font size
     - color
     - alignment (left/center/right)
   - Support “text box” style:
     - Optional background shape behind text (rectangle with color/opacity).

4. SHAPES & HIGHLIGHT BOXES
   - Add “Shape” clips (e.g. rectangle).
   - Properties:
     - color, opacity, border radius
     - position and size
   - Render them behind text to act as highlight boxes.

5. SIMPLE EFFECTS
   - Per-clip:
     - Fade in / fade out
     - Simple zoom in / zoom out
   - These can be implemented as:
     - Clip properties (e.g., fadeInDuration, fadeOutDuration, zoomIn, zoomOut)
     - Or simple time-based transforms (opacity, scale) in the preview and final render.

6. AUDIO CONTROLS
   - Per-clip mute toggle.
   - (Optionally) per-track mute/solo.
   - Volume adjustment per clip (optional later).

7. PREVIEW
   - A React-based preview player that:
     - Reads a unified timeline model.
     - Renders:
       - Video & image layers
       - Text & shape overlays
       - Audio tracks in sync
     - Uses efficient settings suitable for interactive editing.

────────────────────────
PHASED APPROACH — PLAN FIRST, THEN CODE
────────────────────────

You MUST follow a phased workflow:

PHASE 1 — CODEBASE ANALYSIS (NO CODE CHANGES)
---------------------------------------------

Goals:

- Understand VideoSOS:
  - Which components implement:
    - Multi-track timeline UI
    - Tracks and clips
    - Drag & drop, trimming, selection, etc.
  - Which state management solution is used (e.g., Zustand) and where:
    - Project store
    - Timeline store
  - How preview and playback are implemented (Remotion, internal players, etc.).
  - How VideoSOS stores:
    - Projects
    - Timeline data
    - Asset references (e.g., IndexedDB or local storage).

- Understand my app:
  - Asset model:
    - Supabase tables & types
    - Storage paths & URLs
  - Project model:
    - How scripts, audio, images, videos, etc. are organized.
  - Any existing:
    - Library panel components
    - Scene or basic timeline preview components.

Steps:

- Search in `/videosos/src` for editor/timeline-related code:
  - e.g. `Timeline`, `Editor`, `VideoEditor`, `Tracks`, `Track`, `Clip`, `ProjectStore`, etc.
- Identify:
  - The main timeline root component.
  - The state store(s) that represent tracks and clips.
- Inspect my `/src/...` code for:
  - Asset types/interfaces.
  - Supabase integration.
  - Library and project-related components.

Output (in this chat, before any edits):

- A concise summary of:
  - Which VideoSOS components & stores we should reuse or adapt for:
    - Timeline UI
    - Track/clip modeling
    - Preview
  - Which parts of my app are relevant for:
    - Assets
    - Projects
    - Library UI.
- A proposal:
  - Where to create the new editor page (e.g. `/src/app/editor/page.tsx` or similar).
  - Initial mapping of:
    - VideoSOS entities → My app’s entities.

WAIT for my explicit approval before editing any files.

PHASE 2 — INTEGRATION DESIGN
----------------------------

Design a **unified timeline data model** that works for my app.

Requirements:

- Clips must be able to reference:
  - Existing Supabase assets:
    - Video, image, TTS, BGM, SFX, etc.
  - New logical clip types:
    - Text overlays
    - Shape overlays.

- Define TypeScript interfaces, e.g.:
  - `TimelineProject`
  - `TimelineTrack`
  - `TimelineClip`
  - `VideoClip`, `ImageClip`, `AudioClip`, `TextClip`, `ShapeClip`, etc.

- Decide storage:
  - Where and how to store timeline data:
    - In DB tables
    - In JSON columns
    - Or initially in client-side state with later persistence.

- Design a preview architecture:
  - A React-based player that:
    - Uses the timeline model.
    - Can render a simplified preview efficiently.

- Design the final render pipeline:
  - Editor sends timeline JSON + asset URLs to a backend endpoint.
  - Backend uses native FFmpeg (CPU) to produce the final video.
  - Backend stores final video in Supabase (or equivalent) and returns a URL.
  - Frontend polls for job status.

Output:

- A design document in this chat:
  - TS interfaces for timeline entities.
  - Mapping between VideoSOS’s timeline model and my app’s model.
  - UX description:
    - How drag & drop from Library → timeline works.
    - How trim/move/split behave.
  - A small **vertical slice** implementation plan:
    - New editor page.
    - Minimal tracks & preview.

WAIT for my approval before you start coding.

PHASE 3 — MINIMAL VERTICAL SLICE IMPLEMENTATION
-----------------------------------------------

Implement a first working version:

- Create a new editor page in my app, respecting existing routing:
  - e.g. `/src/app/editor/page.tsx`.

- Layout:
  - Left: My existing Asset Library panel (reuse or lightly adapt).
  - Right: Timeline UI + Preview.

- Implement:
  - Load a subset of assets from Supabase.
  - Drag & drop from Library to timeline:
    - On drop, create `TimelineClip` with the right type (video/audio/image).
  - Display a basic timeline:
    - Tracks as rows.
    - Clips as rectangles with durations.
  - Preview:
    - Play/pause.
    - Move playhead.

Constraints:

- Keep changes incremental.
- Use type-safe TS code (no `any`).
- Reuse VideoSOS patterns where suitable, but adapt to my app’s structure and naming conventions.
- Avoid introducing breaking changes to existing flows.

At the end of this phase, summarize:

- What is working.
- What limitations are still present.

PHASE 4 — FEATURE EXPANSION
---------------------------

Gradually add:

4.1 Text overlays:
- Implement `TextClip` type.
- UI to add and edit text clips.
- Preview rendering of text overlays.

4.2 Shape overlays:
- Implement `ShapeClip` type.
- UI to add/edit shape overlays (background boxes, highlight rectangles).
- Render them behind text in preview.

4.3 Simple effects:
- Add fade-in/out and zoom-in/out properties per clip.
- Apply in preview and design the mapping to FFmpeg filters for final render.

4.4 Editing UX:
- Trim handles.
- Split at playhead.
- Drag between tracks.
- Copy/paste.
- Timeline scroll & zoom (mouse wheel, drag on time axis).

4.5 Audio controls:
- Mute/unmute per clip.
- Optional mute/solo per track.

PHASE 5 — POLISH & RENDERING PIPELINE
-------------------------------------

- Add robust error handling:
  - Missing assets
  - Invalid durations
  - Backend render errors.
- Optimize performance:
  - Avoid unnecessary re-renders.
  - Memoize where needed.
- Implement final render pipeline:
  - An “Export” button:
    - Sends timeline JSON + asset URLs to backend.
    - Starts a render job using native FFmpeg (CPU).
    - Polls for job status.
    - Registers final video in my Library.

────────────────────────
GENERAL BEHAVIOR
────────────────────────

- Always think and act like a senior engineer responsible for the whole feature:
  - Architecture
  - Code quality
  - Error handling
  - Performance.
- Use Antigravity tools to:
  - Search the codebase
  - Open and edit files
  - Run dev server / lints / tests, if available.
- Do not tunnel on a single error:
  - If stuck, step back and reassess imports, structure, and assumptions.
- Before any large refactor or cross-cutting change:
  - Write a short “Implementation Plan” in this chat.
  - WAIT for my “OK” before proceeding.




────────────────────────
────────────────────────
────────────────────────
────────────────────────
────────────────────────

FFMPEG & RENDERING GUIDELINES (FLEXIBLE, DEFENSIVE, APP-AWARE)

You are integrating a multi-track timeline editor (inspired by VideoSOS) into my existing app.

This document provides HIGH-LEVEL GUIDANCE for how to design and implement the rendering pipeline, especially around:
- Mapping a timeline JSON model to FFmpeg commands or pipelines.
- Doing this in a way that is SAFE, FLEXIBLE, and RESPECTFUL of the existing app architecture.
- Avoiding brittle or “one-size-fits-all” templates that could conflict with my app’s current behavior.

You MUST treat everything here as guiding principles, not rigid templates.
If anything here conflicts with the actual app architecture or constraints, you should:
- Detect the mismatch,
- Explain it clearly in the chat,
- Propose an adaptation that fits my app better,
- And only then implement changes.

────────────────────────
1. GENERAL PHILOSOPHY
────────────────────────

1.1. SAFETY OVER CLEVERNESS
- Prioritize stability, clarity, and debuggability over “smart” but fragile tricks.
- Never force an FFmpeg pipeline or JSON structure that clearly doesn't align with how my app already stores or uses assets.
- If you are uncertain, propose options in the chat and ask for explicit approval.

1.2. ADAPT TO THE EXISTING ARCHITECTURE
- My app is developed in a “vibe coding” style:
  - The architecture may not be fully documented.
  - Some flows may have implicit assumptions.
- Therefore:
  - Before introducing a new rendering pipeline, carefully inspect:
    - Existing video generation endpoints
    - Existing asset storage (Supabase)
    - Any existing preview/export logic
  - Your design MUST fit into this existing flow, not fight against it.

1.3. LOOSE COUPLING
- Introduce the rendering pipeline as a **loosely coupled service layer**, not as hard-coded FFmpeg calls scattered everywhere.
- For example:
  - Define an interface like `renderTimeline(timeline: TimelineProject): Promise<RenderJobResult>`.
  - Hide all FFmpeg specifics behind this abstraction.
- This makes it easy to:
  - Swap implementations later (local CPU, external service, GPU, etc.).
  - Test the editor without needing full FFmpeg integration at first.

────────────────────────
2. RENDERING LOCATION & RESPONSIBILITY
────────────────────────

2.1. WHERE TO RENDER
- Final rendering MUST NOT happen in the browser using FFmpeg.wasm for full-length/full-quality exports.
- The browser’s role:
  - Provide a timeline editor UI.
  - Provide quick previews (possibly simplified).
  - Send render requests to a backend/worker.
  - Show progress and results.

- The backend/worker’s role:
  - Receive a structured description of the timeline.
  - Orchestrate FFmpeg (native binary, CPU-based is fine).
  - Store the final output (e.g. Supabase Storage).
  - Return a result with a URL and/or job status.

2.2. ASYNC JOB MODEL
- Treat final renders as asynchronous jobs:
  - Request → job ID → periodic progress checks → final result.
- This avoids blocking the UI and keeps the system responsive even on heavy workloads.

────────────────────────
3. TIMELINE JSON → RENDER PIPELINE DESIGN (HIGH LEVEL)
────────────────────────

You DO NOT need to embed a single rigid FFmpeg command here.
Instead, follow these principles:

3.1. SINGLE SOURCE OF TRUTH
- The timeline JSON model should be the **single source of truth** for:
  - What clips exist
  - Where they start/end
  - Which track they belong to (video, image, audio, text, shape)
  - What basic effects they have (fade, zoom, mute, etc.)
- The rendering pipeline should only consume this model, not secret ad-hoc state.

3.2. LAYERED COMPOSITION
- Conceptually, think in layers:

  - Video & Image Layers:
    - Compose background video and overlay images in time.
    - Consider scaling and positioning.
  - Text & Shape Layers:
    - Add text and shapes as overlay layers (e.g. drawtext, boxes).
  - Audio Layers:
    - Mix TTS, BGM, and SFX tracks with proper start times and durations.

- You may implement this as:
  - A filtergraph builder that:
    - Reads the timeline model.
    - Builds a graph of filters (overlay chains, drawtext configs, etc.).
  - Or as:
    - A sequence of simpler FFmpeg calls for specific cases.

3.3. FLEXIBLE MAPPING
- Do NOT hard-code a single giant FFmpeg command that assumes a fixed number of tracks.
- Instead:
  - Programmatically build the pipeline based on:
    - How many clips/tracks are present.
    - Their types and timings.
- If a particular mapping is too complex:
  - Start with a simpler subset (e.g. video + 1 audio + basic text) and explain the limitation.
  - Expand iteratively in later phases.

────────────────────────
4. ERROR HANDLING, DEBUGGING, AND FALLBACKS
────────────────────────

4.1. DEFENSIVE CODING
- Always validate:
  - That asset URLs exist and are reachable before rendering.
  - That clip start/end times are within the overall duration.
  - That there are no negative durations.
- If invalid data is found:
  - Fail fast with a clear error message.
  - Do NOT produce a corrupted video silently.

4.2. LOGGING FOR DEBUGGING
- When invoking FFmpeg:
  - Log the constructed command or arguments in a safe, non-sensitive way.
  - Capture stderr output.
- In development:
  - Return or surface enough debug info so an engineer (or I) can see where things went wrong.
- In production:
  - Avoid leaking sensitive data, but still keep logs accessible for debugging.

4.3. GRACEFUL DEGRADATION
- If some timeline features are not yet supported in the renderer (e.g., advanced effects):
  - Render what you can.
  - Clearly document or log what was ignored or approximated.
- Do NOT crash the entire export just because one optional effect is unsupported.

4.4. SMALL TEST CASES FIRST
- Before attempting complex multi-track, multi-effect exports:
  - Implement and validate:
    - Single video clip export.
    - Video + one audio track.
    - Video + text overlay.
  - Use tiny sample assets and very short durations.
- Only after these pass reliably, gradually increase the complexity.

────────────────────────
5. ADAPTIVE BEHAVIOR & APP INTEGRATION
────────────────────────

5.1. RESPECT EXISTING FLOWS
- Before introducing a new rendering endpoint or service:
  - Inspect how my app currently:
    - Creates videos (if it already does).
    - Stores finished videos in the Library.
  - Design your rendering integration to:
    - Reuse existing patterns.
    - Avoid breaking current users or workflows.

5.2. FEATURE FLAGS / CONFIGURABLE PATHS
- Where possible, introduce new features behind:
  - Feature flags
  - Config flags
- This allows:
  - Progressive rollout.
  - Easy rollback if something goes wrong.

5.3. EXPLAIN YOUR DESIGN
- In this chat, anytime you propose:
  - A new FFmpeg-based pipeline
  - A new endpoint
  - A new JSON structure
- You MUST:
  - Explain your reasoning.
  - Highlight trade-offs (e.g. flexibility vs. complexity).
  - Point out any assumptions you’re making about the existing app.
- If something feels like a guess, say so explicitly and suggest ways to validate it.

────────────────────────
6. WHEN IN DOUBT
────────────────────────

If you are unsure about:

- How to map a specific timeline feature to FFmpeg,
- Whether a particular design will fit my current app structure,
- Or whether a change might break existing flows,

THEN:

1. Do NOT blindly implement it.
2. Instead:
   - Describe the uncertainty in this chat.
   - Propose at least two options:
     - A minimal, safe short-term approach.
     - A more powerful, long-term approach.
   - Ask for my preference before proceeding.

Your goal is to:
- Build a robust, extensible rendering pipeline,
- That works well with my low-spec local environment,
- Leverages server-side/native FFmpeg for performance,
- And fits naturally into my current app’s architecture without causing breakage.
